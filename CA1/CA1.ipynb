{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fffd1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e297c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General help functions\n",
    "# Generate different Q, b, c\n",
    "def get_Q_b_c(n):\n",
    "    Q = np.random.rand(n,n)-0.5\n",
    "    Q = 10*Q @ Q.T # make Q positive definite\n",
    "    b = 5*(np.random.rand(n)-0.5) # b is a vector\n",
    "    c = 2*(np.random.rand(1)-0.5) # c is a scalar\n",
    "    return Q, b, c\n",
    "\n",
    "def cal_f(x, Q, b, c):\n",
    "    # f(x) = 0.5*x^T*Q*x + b^T*x + c\n",
    "    return 0.5 * x.T @ Q @ x + b @ x + c\n",
    "\n",
    "def cal_grad_f(x, Q, b):\n",
    "    # grad_f(x) = Q*x + b\n",
    "    return Q @ x + b\n",
    "\n",
    "# Find the largest eigenvalue of Q\n",
    "def find_max_and_min_eigenvalue(Q):\n",
    "    eigenvalues, _ = np.linalg.eigh(Q) # use eigh for symmetric matrices\n",
    "    return np.max(eigenvalues), np.min(eigenvalues)\n",
    "\n",
    "# Find the inverse of Q\n",
    "def find_inverse(Q):\n",
    "    return np.linalg.inv(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aea62175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Constant step size\n",
    "def grad_descent_with_constant_step(Q, b, c, epsilon, x0=None, step_scale=1, size_type=1):\n",
    "    # clamp step_scale to be in (0,2)\n",
    "    step_scale = max(1e-8, min(step_scale, 2-1e-8))\n",
    "    # compute step size, between (0, 2/L)\n",
    "    L, m = find_max_and_min_eigenvalue(Q)\n",
    "    # choose step size strategy\n",
    "    if size_type == 1:\n",
    "        step_size = step_scale / L\n",
    "    elif size_type == 2:\n",
    "        step_size = m / L**2 # pessimistic and loose step size\n",
    "    elif size_type == 3:\n",
    "        step_size = 2 / (m + L) # optimal constant step size for quadratic functions\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(Q.shape[0])\n",
    "    k = 0 # iteration counter\n",
    "    # calculate gradient at starting point\n",
    "    grad_cur = cal_grad_f(x0, Q, b)\n",
    "    x_cur = x0.copy()\n",
    "    # iterate until the norm of gradient is less than epsilon\n",
    "    while np.linalg.norm(grad_cur, ord=2) >= epsilon:\n",
    "        x_cur -= step_size * grad_cur # gradient descent step\n",
    "        grad_cur = cal_grad_f(x_cur, Q, b)\n",
    "        k += 1 # increment iteration counter\n",
    "    # return x*, k, f(x*), ||grad_f(x*)||_2, L, m\n",
    "    return x_cur, k, cal_f(x_cur, Q, b, c), np.linalg.norm(cal_grad_f(x_cur, Q, b), ord=2), L, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb1fde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2: Armijo's rule\n",
    "def find_alpha_with_armijo(Q, b, c, x_k, beta=0.5):\n",
    "    # s = 1\n",
    "    #Set the adequate parameter. Sigma and beta ranges are specified in the textbook\n",
    "    sigma = 10e-5 # can be between 10^-5 to 10^-1\n",
    "    # beta = 1/2 # can be between 1/10 to 1/2\n",
    "    # L, _ = find_max_and_min_eigenvalue(Q)\n",
    "    alpha = 1\n",
    "\n",
    "    # For easier comparison in the while statement\n",
    "    grad_k = cal_grad_f(x_k, Q, b)\n",
    "    d_k = -grad_k\n",
    "    f_k = cal_f(x_k, Q, b, c)\n",
    "\n",
    "    # So long as f(x+a*d_k) is less than f(x)+sigma*a*grad(f(x))d_k, we keep updating alpha\n",
    "    while cal_f(x_k + alpha * d_k, Q, b, c) > f_k + sigma * alpha * (grad_k @ d_k):\n",
    "        alpha = alpha * beta\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def grad_descent_with_armijo(Q, b, c, epsilon, x0=None, beta=0.5):\n",
    "    # mostly same as part 1 but just the step size is calculated with Armijo.\n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(Q.shape[0])\n",
    "    k = 0 # iteration counter\n",
    "    grad_cur = cal_grad_f(x0, Q, b)\n",
    "    x_cur = x0.copy()\n",
    "    while np.linalg.norm(grad_cur, ord=2) >= epsilon:\n",
    "        step_size = find_alpha_with_armijo(Q, b, c, x_cur, beta=beta)\n",
    "        # print(f\"armijo found={step_size}, K={k}, {np.linalg.norm(grad_cur, ord=2)}, {epsilon}\")\n",
    "        x_cur -= step_size * grad_cur\n",
    "        grad_cur = cal_grad_f(x_cur, Q, b)\n",
    "        k += 1\n",
    "    # return x*, k, f(x*), ||grad_f(x*)||_2, L\n",
    "    return x_cur, k, cal_f(x_cur, Q, b, c), np.linalg.norm(cal_grad_f(x_cur, Q, b), ord=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18ba72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Matrix Inversion\n",
    "def solve_by_matrix_inversion(Q, b, c):\n",
    "    x_star = -find_inverse(Q) @ b # x* = -Q^(-1)b\n",
    "    f_star = cal_f(x_star, Q, b, c) # f* = f(x*)\n",
    "    return x_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e4daae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parameters ===\n",
      "seed = 114, n = 10, epsilon = 1e-05\n",
      "\n",
      "=== Task1: Constant Step Size Gradient Descent ===\n",
      "L  = 27.811818801552185, m = 0.3958415370832774, step_scale = 1.0\n",
      "iterations = 787\n",
      "f(x*) = -4.211898464143623\n",
      "x* = [ 0.51567481  0.47366284 -0.46934419  0.01249601 -1.11731404 -1.1317547\n",
      " -0.46193719  0.84590148 -1.52789913  0.8710842 ]\n",
      "||grad_f(x*)||_2 = 9.88070172462399e-06\n",
      "\n",
      "=== Task2: Backtracking Line Search ===\n",
      "beta = 0.5\n",
      "iterations = 381\n",
      "f(x*) = -4.211898464210382\n",
      "x* = [ 0.51567525  0.4736608  -0.46934358  0.01249914 -1.11731639 -1.13176022\n",
      " -0.46193779  0.84590522 -1.52790023  0.87108491]\n",
      "||grad_f(x*)||_2 = 8.967992535110175e-06\n",
      "\n",
      "=== Task3: Matrix Inversion ===\n",
      "f(x*) = -4.211898464266943\n",
      "x* = [ 0.51567596  0.47365676 -0.46934266  0.01250579 -1.11732094 -1.13177153\n",
      " -0.46193916  0.84591313 -1.52790235  0.87108662]\n",
      "\n",
      "=== Comparison of Different Step Size Strategies ===\n",
      "\n",
      "=== Step Size = step_scale / L ===\n",
      "iterations = 787\n",
      "f(x*) = -4.211898464143623\n",
      "x* = [ 0.51567481  0.47366284 -0.46934419  0.01249601 -1.11731404 -1.1317547\n",
      " -0.46193719  0.84590148 -1.52789913  0.8710842 ]\n",
      "||grad_f(x*)||_2 = 9.88070172462399e-06\n",
      "\n",
      "=== Step Size = m / L^2 ===\n",
      "iterations = 55627\n",
      "f(x*) = -4.211898464140626\n",
      "x* = [ 0.5156748   0.47366291 -0.46934421  0.01249589 -1.11731396 -1.1317545\n",
      " -0.46193717  0.84590134 -1.52789909  0.87108417]\n",
      "||grad_f(x*)||_2 = 9.999997241912136e-06\n",
      "\n",
      "=== Step Size = 2 / (m + L) ===\n",
      "iterations = 516\n",
      "f(x*) = -4.211898464265099\n",
      "x* = [ 0.51567583  0.47365705 -0.46934289  0.01250557 -1.11732056 -1.13177095\n",
      " -0.46193916  0.84591286 -1.52790216  0.87108666]\n",
      "||grad_f(x*)||_2 = 9.749503838847943e-06\n"
     ]
    }
   ],
   "source": [
    "# For Testing\n",
    "if __name__ == \"__main__\":\n",
    "    seed = 114\n",
    "    np.random.seed(seed)\n",
    "    n = 10\n",
    "    epsilon = 1e-5\n",
    "    Q, b, c = get_Q_b_c(n)\n",
    "    x0 = np.random.rand(n)\n",
    "    # m/L^2 is a pessimistic and loose step size for all functions satisfying\n",
    "    # m-strong convexity and L-smoothness\n",
    "    # 2 / (m + L) is the optimal constant step size specific for quadratic functions\n",
    "    step_scale = 1 # can be between (0,2), for task 1\n",
    "    beta = 0.5 # theoretically can be between (0,1), for task 2\n",
    "    # Task 1\n",
    "    x_task1, k_task1, f_task1, grad_norm_task1, L, m = grad_descent_with_constant_step(Q, b, c, epsilon, x0, step_scale, 1)\n",
    "    # Task 2\n",
    "    x_task2, k_task2, f_task2, grad_norm_task2 = grad_descent_with_armijo(Q, b, c, epsilon, x0, beta)\n",
    "    # Task 3\n",
    "    x_task3, f_task3 = solve_by_matrix_inversion(Q, b, c)\n",
    "\n",
    "    # print parameters\n",
    "    print(\"=== Parameters ===\")\n",
    "    print(f\"seed = {seed}, n = {n}, epsilon = {epsilon}\")\n",
    "    \n",
    "    # print task 1\n",
    "    print(\"\\n=== Task1: Constant Step Size Gradient Descent ===\")\n",
    "    print(f\"L  = {float(L)}, m = {float(m)}, step_scale = {float(step_scale)}\")\n",
    "    print(f\"iterations = {k_task1}\")\n",
    "    print(f\"f(x*) = {float(f_task1.item())}\")\n",
    "    print(f\"x* = {x_task1.astype(float)}\")\n",
    "    print(f\"||grad_f(x*)||_2 = {float(grad_norm_task1)}\")\n",
    "\n",
    "    # print task 2\n",
    "    print(\"\\n=== Task2: Backtracking Line Search ===\")\n",
    "    print(f\"beta = {float(beta)}\")\n",
    "    print(f\"iterations = {k_task2}\")\n",
    "    print(f\"f(x*) = {float(f_task2.item())}\")\n",
    "    print(f\"x* = {x_task2.astype(float)}\")\n",
    "    print(f\"||grad_f(x*)||_2 = {float(grad_norm_task2)}\")\n",
    "\n",
    "    # print task 3\n",
    "    print(\"\\n=== Task3: Matrix Inversion ===\")\n",
    "    print(f\"f(x*) = {float(f_task3.item())}\")\n",
    "    print(f\"x* = {x_task3.astype(float)}\")\n",
    "\n",
    "\n",
    "    # Make comparisons between different step size strategies\n",
    "    # Type 1 has been done above\n",
    "    # Type 2: m/L^2\n",
    "    x_task1_2, k_task1_2, f_task1_2, grad_norm_task1_2, _, _ = grad_descent_with_constant_step(Q, b, c, epsilon, x0, step_scale, 2)\n",
    "    # Type 3: 2/(m+L)\n",
    "    x_task1_3, k_task1_3, f_task1_3, grad_norm_task1_3, _, _ = grad_descent_with_constant_step(Q, b, c, epsilon, x0, step_scale, 3)\n",
    "    # print comparisons\n",
    "    print(\"\\n=== Comparison of Different Step Size Strategies ===\")\n",
    "    print(\"\\n=== Step Size = step_scale / L ===\")\n",
    "    print(f\"iterations = {k_task1}\")\n",
    "    print(f\"f(x*) = {float(f_task1.item())}\")\n",
    "    print(f\"x* = {x_task1.astype(float)}\")\n",
    "    print(f\"||grad_f(x*)||_2 = {float(grad_norm_task1)}\")\n",
    "    # m/L^2\n",
    "    print(\"\\n=== Step Size = m / L^2 ===\")\n",
    "    print(f\"iterations = {k_task1_2}\")\n",
    "    print(f\"f(x*) = {float(f_task1_2.item())}\")\n",
    "    print(f\"x* = {x_task1_2.astype(float)}\")\n",
    "    print(f\"||grad_f(x*)||_2 = {float(grad_norm_task1_2)}\")\n",
    "    # 2/(m+L)\n",
    "    print(\"\\n=== Step Size = 2 / (m + L) ===\")\n",
    "    print(f\"iterations = {k_task1_3}\")\n",
    "    print(f\"f(x*) = {float(f_task1_3.item())}\")\n",
    "    print(f\"x* = {x_task1_3.astype(float)}\")\n",
    "    print(f\"||grad_f(x*)||_2 = {float(grad_norm_task1_3)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
